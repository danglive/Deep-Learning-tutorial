{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danglive/Deep-Learning-tutorial/blob/master/CNN%20tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj9iQ3yXBK0t",
        "colab_type": "text"
      },
      "source": [
        "##Input data and placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu0UnerlBN5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# Python optimisation variables\n",
        "learning_rate = 0.0001\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "\n",
        "# declare the training data placeholders\n",
        "# input x - for 28 x 28 pixels = 784 - this is the flattened image data that is drawn from \n",
        "# mnist.train.nextbatch()\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "# dynamically reshape the input\n",
        "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
        "# now declare the output data placeholder - 10 digits\n",
        "y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlOD03X-BhCZ",
        "colab_type": "text"
      },
      "source": [
        "##Defining the convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGwCTAaHBig-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
        "    # setup the filter input shape for tf.nn.conv_2d\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
        "                      num_filters]\n",
        "\n",
        "    # initialise weights and bias for the filter\n",
        "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
        "                                      name=name+'_W')\n",
        "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
        "\n",
        "    # setup the convolutional layer operation\n",
        "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "    # add the bias\n",
        "    out_layer += bias\n",
        "\n",
        "    # apply a ReLU non-linear activation\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "\n",
        "    # now perform max pooling\n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
        "                               padding='SAME')\n",
        "\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd5xes9HBo5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
        "                      num_filters]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcemkCm9Br7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialise weights and bias for the filter\n",
        "weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
        "                                  name=name+'_W')\n",
        "bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp6BsYmNBvbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup the convolutional layer operation\n",
        "out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOX_c04-By7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the bias\n",
        "out_layer += bias\n",
        "# apply a ReLU non-linear activation\n",
        "out_layer = tf.nn.relu(out_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsmAPfZsB1Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now perform max pooling\n",
        "ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "strides = [1, 2, 2, 1]\n",
        "out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
        "                               padding='SAME')\n",
        "\n",
        "return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28qRPZO_B4Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create some convolutional layers\n",
        "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
        "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMMQuKoAB4wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDFgUq1bB619",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup some weights and bias values for this layer, then activate with ReLU\n",
        "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
        "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
        "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
        "dense_layer1 = tf.nn.relu(dense_layer1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72A8CY9pB_B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another layer with softmax activations\n",
        "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
        "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
        "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
        "y_ = tf.nn.softmax(dense_layer2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnJAuSWLBl0K",
        "colab_type": "text"
      },
      "source": [
        "##The cross-entropy cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo7CZ5rjCCr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npPDxoNBCL_u",
        "colab_type": "text"
      },
      "source": [
        "##The training of the convolutional neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHHXn3uICWAo",
        "colab_type": "text"
      },
      "source": [
        " Weâ€™ll be using mini-batches to train our network.  The essential structure is:\n",
        "\n",
        "Create an optimiser\n",
        "\n",
        "Create correct prediction and accuracy evaluation operations\n",
        "\n",
        "Initialise the operations\n",
        "\n",
        "Determine the number of batch runs within an training epoch\n",
        "\n",
        "For each epoch:\n",
        "\n",
        "For each batch:\n",
        "\n",
        "Extract the batch data\n",
        "\n",
        "Run the optimiser and cross-entropy operations\n",
        "\n",
        "Add to the average cost\n",
        "\n",
        "Calculate the current test accuracy\n",
        "\n",
        "Print out some results\n",
        "\n",
        "Calculate the final test accuracy and print"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q1pLOpBCJbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add an optimiser\n",
        "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "# define an accuracy assessment operation\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# setup the initialisation operator\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # initialise the variables\n",
        "    sess.run(init_op)\n",
        "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "            _, c = sess.run([optimiser, cross_entropy], \n",
        "                            feed_dict={x: batch_x, y: batch_y})\n",
        "            avg_cost += c / total_batch\n",
        "        test_acc = sess.run(accuracy, \n",
        "                       feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" \n",
        "                 test accuracy: {:.3f}\".format(test_acc))\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}